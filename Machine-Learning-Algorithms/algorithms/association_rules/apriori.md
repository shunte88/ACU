# Apriori Algorithm

## 1. Overview
The Apriori algorithm is a classic algorithm for finding frequent itemsets and generating association rules in transactional databases. It uses a "bottom-up" approach where frequent subsets are extended one item at a time.

### Type of Learning
- Unsupervised Learning
- Association Rule Learning
- Market Basket Analysis

### Key Characteristics
- Discovers frequent itemsets
- Generates association rules
- Uses support and confidence metrics
- Implements the Apriori principle
- Scalable for large datasets

### When to Use
- Market basket analysis
- Product recommendation
- Cross-selling strategies
- Customer behavior analysis
- Pattern discovery in transactions

## 2. Historical Context
- Developed by Agrawal and Srikant in 1994
- One of the first algorithms for association rule mining
- Still widely used in practice
- Foundation for many other algorithms

## 3. Technical Details

### Mathematical Foundation

#### Support
The support of an itemset X is the proportion of transactions that contain X:
$$
\text{support}(X) = \frac{|\{t \in T | X \subseteq t\}|}{|T|}
$$
where:
- $T$ is the set of all transactions
- $t$ is a single transaction
- $|T|$ is the total number of transactions

#### Confidence
The confidence of a rule $X \rightarrow Y$ is the proportion of transactions containing X that also contain Y:
$$
\text{confidence}(X \rightarrow Y) = \frac{\text{support}(X \cup Y)}{\text{support}(X)}
$$

#### Lift
The lift of a rule measures how much more likely Y is when X is present:
$$
\text{lift}(X \rightarrow Y) = \frac{\text{confidence}(X \rightarrow Y)}{\text{support}(Y)} = \frac{\text{support}(X \cup Y)}{\text{support}(X) \times \text{support}(Y)}
$$

#### Apriori Principle
If an itemset is frequent, then all its subsets must also be frequent:
$$
\text{If } X \subseteq Y \text{ and } \text{support}(Y) \geq \text{min\_support} \text{ then } \text{support}(X) \geq \text{min\_support}
$$

#### Candidate Generation
For k-itemsets, candidates are generated by joining (k-1)-itemsets:
$$
C_k = \{X \cup Y | X, Y \in L_{k-1}, |X \cap Y| = k-2\}
$$
where:
- $C_k$ is the set of candidate k-itemsets
- $L_{k-1}$ is the set of frequent (k-1)-itemsets

### Training Process
1. Generate frequent 1-itemsets
2. Generate candidate k-itemsets
3. Prune infrequent itemsets
4. Generate association rules
5. Filter rules by confidence

### Key Parameters
- Minimum support threshold
- Minimum confidence threshold
- Minimum lift threshold
- Maximum itemset size

## 4. Performance Analysis

### Time Complexity
The time complexity can be broken down into several components:

1. Candidate Generation:
$$
O(\sum_{k=1}^{n} C(n,k)) = O(2^n)
$$
where n is the number of items

2. Support Counting:
$$
O(m \times \sum_{k=1}^{n} C(n,k)) = O(m \times 2^n)
$$
where m is the number of transactions

3. Rule Generation:
$$
O(|L| \times 2^{|L|})
$$
where |L| is the number of frequent itemsets

In practice, with pruning and optimizations:
$$
O(m \times n \times k)
$$
where:
- m = number of transactions
- n = number of items
- k = maximum itemset size

### Space Complexity
1. Transaction Storage:
$$
O(m \times n)
$$

2. Candidate Itemsets:
$$
O(\sum_{k=1}^{n} C(n,k)) = O(2^n)
$$

3. Frequent Itemsets:
$$
O(|L|)
$$
where |L| is the number of frequent itemsets

### Computational Requirements
- Memory intensive
- Requires multiple passes
- Can be optimized
- Suitable for distributed computing

## 5. Practical Applications
- Market basket analysis
- Product recommendations
- Cross-selling strategies
- Customer behavior analysis
- Web usage mining
- Intrusion detection
- Medical diagnosis

## 6. Advantages and Limitations

### Advantages
- Simple to understand and implement
- Works well with sparse data
- Can find all frequent itemsets
- Provides clear association rules
- Handles categorical data well

### Limitations
- Computationally expensive
- Memory intensive
- Multiple database scans
- Sensitive to minimum support
- Can generate many rules

## 7. Comparison with Similar Algorithms

### vs FP-Growth
- **Apriori**: Multiple database scans, candidate generation
- **FP-Growth**: Single database scan, pattern growth
- **Use Case**: Choose FP-Growth for better performance

### vs ECLAT
- **Apriori**: Horizontal data format, breadth-first
- **ECLAT**: Vertical data format, depth-first
- **Use Case**: Choose based on data structure

### vs Apriori-TID
- **Apriori**: Original algorithm, multiple scans
- **Apriori-TID**: Transaction ID lists, fewer scans
- **Use Case**: Choose Apriori-TID for better efficiency

### vs Partition
- **Apriori**: Global frequent itemsets
- **Partition**: Local frequent itemsets
- **Use Case**: Choose based on data distribution

### vs Sampling
- **Apriori**: Exact results, complete search
- **Sampling**: Approximate results, faster
- **Use Case**: Choose based on accuracy requirements

## 8. Implementation Guidelines

### Prerequisites
- NumPy
- Pandas
- Matplotlib
- Seaborn

### Data Requirements
- Transactional data
- Binary or categorical features
- Clean data
- Appropriate encoding

### Best Practices
- Choose appropriate thresholds
- Preprocess data carefully
- Handle missing values
- Consider data sparsity
- Use efficient data structures

## 9. Python Implementation
See `apriori.py` for complete implementation. 