{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation-Maximization (EM) Clustering Implementation<br>\n",
    "This notebook demonstrates the implementation of the EM algorithm for Gaussian Mixture Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Table of Contents<br>\n",
    "1. Import Required Libraries<br>\n",
    "2. Data Generation and Preprocessing<br>\n",
    "3. EM Implementation<br>\n",
    "4. Model Training and Evaluation<br>\n",
    "5. Visualization<br>\n",
    "6. Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Data Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=300, n_features=2, centers=3, cluster_std=1.0):\n",
    "    \"\"\"\n",
    "    Generate sample data for clustering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int, default=300\n",
    "        Number of samples to generate\n",
    "    n_features : int, default=2\n",
    "        Number of features\n",
    "    centers : int, default=3\n",
    "        Number of clusters\n",
    "    cluster_std : float, default=1.0\n",
    "        Standard deviation of clusters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        X : numpy.ndarray\n",
    "            Features array of shape (n_samples, n_features)\n",
    "        y : numpy.ndarray\n",
    "            True cluster labels\n",
    "    \"\"\"\n",
    "    X, y = make_blobs(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        centers=centers,\n",
    "        cluster_std=cluster_std,\n",
    "        random_state=2220\n",
    "    )\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. EM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X, n_components, random_state=None):\n",
    "    \"\"\"\n",
    "    Initialize GMM parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Input data\n",
    "    n_components : int\n",
    "        Number of mixture components\n",
    "    random_state : int, default=None\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        means : numpy.ndarray\n",
    "            Initial means\n",
    "        covariances : numpy.ndarray\n",
    "            Initial covariances\n",
    "        weights : numpy.ndarray\n",
    "            Initial mixture weights\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize means using k-means++ strategy\n",
    "    np.random.seed(random_state)\n",
    "    means = X[np.random.choice(n_samples, n_components, replace=False)]\n",
    "    \n",
    "    # Initialize covariances as identity matrices\n",
    "    covariances = np.array([np.eye(n_features) for _ in range(n_components)])\n",
    "    \n",
    "    # Initialize weights uniformly\n",
    "    weights = np.ones(n_components) / n_components\n",
    "    \n",
    "    return means, covariances, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(X, means, covariances, weights):\n",
    "    \"\"\"\n",
    "    Expectation step: compute responsibilities.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Input data\n",
    "    means : numpy.ndarray\n",
    "        Current means\n",
    "    covariances : numpy.ndarray\n",
    "        Current covariances\n",
    "    weights : numpy.ndarray\n",
    "        Current mixture weights\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Responsibilities matrix\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_components = len(weights)\n",
    "    responsibilities = np.zeros((n_samples, n_components))\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        responsibilities[:, k] = weights[k] * multivariate_normal.pdf(\n",
    "            X, mean=means[k], cov=covariances[k]\n",
    "        )\n",
    "        \n",
    "    # Normalize responsibilities\n",
    "    responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return responsibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(X, responsibilities):\n",
    "    \"\"\"\n",
    "    Maximization step: update parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Input data\n",
    "    responsibilities : numpy.ndarray\n",
    "        Responsibilities matrix\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        means : numpy.ndarray\n",
    "            Updated means\n",
    "        covariances : numpy.ndarray\n",
    "            Updated covariances\n",
    "        weights : numpy.ndarray\n",
    "            Updated mixture weights\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components = responsibilities.shape[1]\n",
    "    \n",
    "    # Update weights\n",
    "    weights = responsibilities.sum(axis=0) / n_samples\n",
    "    \n",
    "    # Update means\n",
    "    means = np.zeros((n_components, n_features))\n",
    "    for k in range(n_components):\n",
    "        means[k] = np.average(X, axis=0, weights=responsibilities[:, k])\n",
    "        \n",
    "    # Update covariances\n",
    "    covariances = np.zeros((n_components, n_features, n_features))\n",
    "    for k in range(n_components):\n",
    "        diff = X - means[k]\n",
    "        covariances[k] = np.average(\n",
    "            diff[:, :, np.newaxis] * diff[:, np.newaxis, :],\n",
    "            axis=0,\n",
    "            weights=responsibilities[:, k]\n",
    "        )\n",
    "        \n",
    "        # Add small value to diagonal for numerical stability\n",
    "        covariances[k] += np.eye(n_features) * 1e-6\n",
    "        \n",
    "    return means, covariances, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(X, means, covariances, weights):\n",
    "    \"\"\"\n",
    "    Compute log likelihood of the data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Input data\n",
    "    means : numpy.ndarray\n",
    "        Current means\n",
    "    covariances : numpy.ndarray\n",
    "        Current covariances\n",
    "    weights : numpy.ndarray\n",
    "        Current mixture weights\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Log likelihood\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_components = len(weights)\n",
    "    log_likelihood = 0\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        log_likelihood += weights[k] * multivariate_normal.pdf(\n",
    "            X, mean=means[k], cov=covariances[k]\n",
    "        )\n",
    "        \n",
    "    return np.sum(np.log(log_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_clustering(X, n_components=3, max_iter=100, tol=1e-4, random_state=None):\n",
    "    \"\"\"\n",
    "    Perform EM clustering using Gaussian Mixture Models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Input data of shape (n_samples, n_features)\n",
    "    n_components : int, default=3\n",
    "        Number of mixture components\n",
    "        - Determines the number of clusters\n",
    "        - Should be chosen based on domain knowledge\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of EM iterations\n",
    "        - More iterations can lead to better convergence\n",
    "        - May cause overfitting if too high\n",
    "    tol : float, default=1e-4\n",
    "        Convergence threshold\n",
    "        - Smaller values lead to more precise convergence\n",
    "        - Larger values may cause early stopping\n",
    "    random_state : int, default=None\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        labels : numpy.ndarray\n",
    "            Cluster labels for each sample\n",
    "        means : numpy.ndarray\n",
    "            Final means\n",
    "        covariances : numpy.ndarray\n",
    "            Final covariances\n",
    "        weights : numpy.ndarray\n",
    "            Final mixture weights\n",
    "        log_likelihood : float\n",
    "            Final log likelihood\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    means, covariances, weights = initialize_parameters(X, n_components, random_state)\n",
    "    \n",
    "    # EM iterations\n",
    "    prev_log_likelihood = -np.inf\n",
    "    for iteration in range(max_iter):\n",
    "        # E-step\n",
    "        responsibilities = e_step(X, means, covariances, weights)\n",
    "        \n",
    "        # M-step\n",
    "        means, covariances, weights = m_step(X, responsibilities)\n",
    "        \n",
    "        # Compute log likelihood\n",
    "        log_likelihood = compute_log_likelihood(X, means, covariances, weights)\n",
    "        \n",
    "        # Check convergence\n",
    "        if abs(log_likelihood - prev_log_likelihood) < tol:\n",
    "            break\n",
    "            \n",
    "        prev_log_likelihood = log_likelihood\n",
    "    \n",
    "    # Assign labels based on responsibilities\n",
    "    labels = np.argmax(responsibilities, axis=1)\n",
    "    \n",
    "    return labels, means, covariances, weights, log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(X, labels):\n",
    "    \"\"\"\n",
    "    Evaluate clustering results using various metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Input data\n",
    "    labels : numpy.ndarray\n",
    "        Cluster labels\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'silhouette_score': silhouette_score(X, labels),\n",
    "        'calinski_harabasz_score': calinski_harabasz_score(X, labels)\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, labels, means=None, covariances=None, title=None):\n",
    "    \"\"\"\n",
    "    Plot clustering results using seaborn.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Input data\n",
    "    labels : numpy.ndarray\n",
    "        Cluster labels\n",
    "    means : numpy.ndarray, default=None\n",
    "        Cluster means\n",
    "    covariances : numpy.ndarray, default=None\n",
    "        Cluster covariances\n",
    "    title : str, default=None\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, palette='deep')\n",
    "    \n",
    "    if means is not None:\n",
    "        plt.scatter(means[:, 0], means[:, 1], c='red', marker='x', s=100, label='Centroids')\n",
    "        \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 6. Results and Analysis<br>\n",
    "Generate sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, true_labels = generate_data(n_samples=300, n_features=2, centers=3, cluster_std=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform EM clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, means, covariances, weights, log_likelihood = em_clustering(\n",
    "    X,\n",
    "    n_components=3,\n",
    "    max_iter=100,\n",
    "    tol=1e-4,\n",
    "    random_state=2220\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering Evaluation Metrics:\n",
      "silhouette_score: 0.8231\n",
      "calinski_harabasz_score: 4548.4910\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_clustering(X, labels)\n",
    "print(\"\\nClustering Evaluation Metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEM Clustering Results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mplot_clusters\u001b[39m\u001b[34m(X, labels, means, covariances, title)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_clusters\u001b[39m(X, labels, means=\u001b[38;5;28;01mNone\u001b[39;00m, covariances=\u001b[38;5;28;01mNone\u001b[39;00m, title=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Plot clustering results using seaborn.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33;03m        Plot title\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m     19\u001b[39m     sns.scatterplot(x=X[:, \u001b[32m0\u001b[39m], y=X[:, \u001b[32m1\u001b[39m], hue=labels, palette=\u001b[33m'\u001b[39m\u001b[33mdeep\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m means \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plot_clusters(X, labels, means, covariances, \"EM Clustering Results\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
