# Support Vector Machines (SVM)

## 1. Overview
Support Vector Machines are powerful supervised learning models that find optimal hyperplanes to separate classes in high-dimensional spaces. They maximize the margin between classes while minimizing classification errors.

### Type of Learning
- Supervised Learning
- Maximum Margin Classifier
- Classification and Regression Tasks

### Key Characteristics
- Margin maximization
- Kernel trick
- Non-linear classification
- Regularization
- Sparse solution
- Robust to high dimensions

### When to Use
- When you have clear margin of separation
- When data is high-dimensional
- When number of features > number of samples
- When you need non-linear classification
- When you want to avoid overfitting
- When you need a robust classifier

## 2. Historical Context
- Developed by Vladimir Vapnik and Alexey Chervonenkis in 1963
- Kernel trick introduced by Bernhard Boser et al. in 1992
- Soft margin SVM proposed by Corinna Cortes and Vapnik in 1995
- Foundation for many modern machine learning methods

## 3. Technical Details

### Mathematical Foundation

#### Linear SVM
For a dataset $\{(x_i, y_i)\}_{i=1}^n$ where $y_i \in \{-1, 1\}$, the optimization problem is:

$$
\min_{w,b} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^n \xi_i
$$

subject to:
$$
y_i(w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
$$

where:
- $w$ is the weight vector
- $b$ is the bias term
- $\xi_i$ are slack variables
- $C$ is the regularization parameter

#### Dual Form
The dual optimization problem is:
$$
\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j
$$

subject to:
$$
0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
$$

#### Kernel Trick
Replace dot product with kernel function:
$$
K(x_i, x_j) = \phi(x_i)^T \phi(x_j)
$$

Common kernels:
1. Linear: $K(x_i, x_j) = x_i^T x_j$
2. Polynomial: $K(x_i, x_j) = (\gamma x_i^T x_j + r)^d$
3. RBF: $K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$
4. Sigmoid: $K(x_i, x_j) = \tanh(\gamma x_i^T x_j + r)$

#### Decision Function
The final decision function is:
$$
f(x) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i K(x_i, x) + b\right)
$$

### Training Process
1. Choose kernel and parameters
2. Solve dual optimization problem
3. Extract support vectors
4. Compute bias term
5. Form decision function

### Key Parameters
- Kernel type
- Regularization parameter (C)
- Kernel parameters (Î³, d, r)
- Tolerance
- Maximum iterations
- Class weights

## 4. Performance Analysis

### Time Complexity
- **Training:**
  - Linear SVM: $O(n \times d)$
  - Kernel SVM: $O(n^2 \times d)$ to $O(n^3 \times d)$
  where:
  - $n$ = number of samples
  - $d$ = number of features

- **Prediction:**
  - Linear SVM: $O(d)$
  - Kernel SVM: $O(n_{sv} \times d)$
  where $n_{sv}$ is number of support vectors

### Space Complexity
- **Training:**
  - Linear SVM: $O(n \times d)$
  - Kernel SVM: $O(n^2)$ for kernel matrix

- **Prediction:**
  - Linear SVM: $O(d)$
  - Kernel SVM: $O(n_{sv} \times d)$

### Computational Requirements
- Memory intensive for kernel SVM
- Benefits from feature scaling
- Parallelizable for linear SVM
- Sensitive to parameter tuning
- Can be accelerated with approximations

### Scalability Analysis
- Training time scales with:
  - Dataset size
  - Feature dimensionality
  - Kernel complexity
- Memory usage scales with:
  - Dataset size
  - Kernel matrix size
- Prediction time scales with:
  - Number of support vectors
  - Feature dimensionality

## 5. Practical Applications
- Text classification
- Image recognition
- Bioinformatics
- Handwriting recognition
- Face detection
- Protein classification
- Financial forecasting

## 6. Advantages and Limitations

### Advantages
- Effective in high dimensions
- Memory efficient (uses support vectors)
- Versatile (different kernels)
- Robust to overfitting
- Clear geometric interpretation
- Works well with small datasets

### Limitations
- Sensitive to parameter tuning
- Computationally expensive for large datasets
- Memory intensive for kernel SVM
- Requires feature scaling
- Binary classification by default
- Can be sensitive to noise

## 7. Implementation Guidelines

### Prerequisites
- NumPy
- Pandas
- Scikit-learn
- Matplotlib (for visualization)

### Data Requirements
- Clean data
- Relevant features
- Proper scaling
- No missing values
- Binary classification (or one-vs-rest)
- Sufficient training data

### Best Practices
- Feature scaling
- Cross-validation
- Kernel selection
- Parameter tuning
- Proper evaluation metrics
- Support vector analysis

## 8. Python Implementation
See `SVM.py` for complete implementation. 