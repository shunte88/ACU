{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Support Vector Machines (SVM) Implementation<br>\n",
    "This notebook demonstrates both custom and scikit-learn implementations of SVM.<br>\n",
    "SVM is a powerful supervised learning algorithm used for classification and regression tasks.<br>\n",
    "It finds the optimal hyperplane that maximizes the margin between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Data Generation and Preprocessing<br>\n",
    "Generate synthetic data for binary classification<br>\n",
    "The data consists of two classes with Gaussian distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    \"\"\"\n",
    "    Generate sample data for binary classification.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y) features and target\n",
    "    \"\"\"\n",
    "    n_samples = 100\n",
    "    \n",
    "    # Generate two classes\n",
    "    X1 = np.random.randn(n_samples//2, 2) + np.array([2, 2])\n",
    "    X2 = np.random.randn(n_samples//2, 2) + np.array([-2, -2])\n",
    "    X = np.vstack([X1, X2])\n",
    "    y = np.hstack([np.ones(n_samples//2), -np.ones(n_samples//2)])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. Custom SVM Implementation<br>\n",
    "The custom implementation uses quadratic programming to find the optimal hyperplane<br>\n",
    "It supports different kernel functions for handling non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_function(x1, x2, kernel='linear', gamma='scale', degree=3, coef0=0.0):\n",
    "    \"\"\"\n",
    "    Compute kernel function between two points.\n",
    "    \n",
    "    Hyperparameters:\n",
    "    - kernel (str): Type of kernel function ('linear', 'poly', 'rbf', 'sigmoid').\n",
    "      Different kernels can capture different types of decision boundaries.\n",
    "    - gamma (float): Kernel coefficient for 'rbf', 'poly', 'sigmoid' kernels.\n",
    "      Controls the influence of individual training samples.\n",
    "    - degree (int): Degree of polynomial kernel. Higher degrees can capture more\n",
    "      complex decision boundaries but may lead to overfitting.\n",
    "    - coef0 (float): Independent term in kernel function. Used in polynomial and\n",
    "      sigmoid kernels.\n",
    "    \n",
    "    Args:\n",
    "        x1 (numpy.ndarray): First point\n",
    "        x2 (numpy.ndarray): Second point\n",
    "        kernel (str): Type of kernel\n",
    "        gamma (float): Kernel coefficient\n",
    "        degree (int): Degree of polynomial kernel\n",
    "        coef0 (float): Independent term in kernel function\n",
    "        \n",
    "    Returns:\n",
    "        float: Kernel value\n",
    "    \"\"\"\n",
    "    if kernel == 'linear':\n",
    "        return np.dot(x1, x2)\n",
    "    elif kernel == 'poly':\n",
    "        return (gamma * np.dot(x1, x2) + coef0) ** degree\n",
    "    elif kernel == 'rbf':\n",
    "        return np.exp(-gamma * np.sum((x1 - x2) ** 2))\n",
    "    elif kernel == 'sigmoid':\n",
    "        return np.tanh(gamma * np.dot(x1, x2) + coef0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel: {kernel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel_matrix(X, kernel='linear', gamma='scale', degree=3, coef0=0.0):\n",
    "    \"\"\"\n",
    "    Compute kernel matrix for training data.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Training features\n",
    "        kernel (str): Type of kernel\n",
    "        gamma (float): Kernel coefficient\n",
    "        degree (int): Degree of polynomial kernel\n",
    "        coef0 (float): Independent term in kernel function\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Kernel matrix\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    K = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            K[i, j] = kernel_function(X[i], X[j], kernel, gamma, degree, coef0)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_svm_fit(X, y, kernel='linear', C=1.0, gamma='scale', degree=3, coef0=0.0):\n",
    "    \"\"\"\n",
    "    Train an SVM model using quadratic programming.\n",
    "    \n",
    "    Hyperparameters:\n",
    "    - C (float): Regularization parameter. Controls the trade-off between having\n",
    "      a large margin and ensuring that points lie on the correct side of the margin.\n",
    "      Higher values of C lead to a smaller margin but better classification of\n",
    "      training points.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Training features\n",
    "        y (numpy.ndarray): Target values (-1 or 1)\n",
    "        kernel (str): Type of kernel\n",
    "        C (float): Regularization parameter\n",
    "        gamma (float): Kernel coefficient\n",
    "        degree (int): Degree of polynomial kernel\n",
    "        coef0 (float): Independent term in kernel function\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (alpha, support_vectors, support_vector_labels, b) trained parameters\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Compute kernel matrix\n",
    "    K = compute_kernel_matrix(X, kernel, gamma, degree, coef0)\n",
    "\n",
    "    # Set up quadratic programming problem\n",
    "    P = matrix(np.outer(y, y) * K)\n",
    "    q = matrix(-np.ones(n_samples))\n",
    "    G = matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))\n",
    "    h = matrix(np.hstack((np.zeros(n_samples), C * np.ones(n_samples))))\n",
    "    A = matrix(y.astype(float), (1, n_samples))\n",
    "    b = matrix(0.0)\n",
    "\n",
    "    # Solve quadratic programming problem\n",
    "    solvers.options['show_progress'] = False\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "    alpha = np.array(solution['x']).flatten()\n",
    "\n",
    "    # Find support vectors\n",
    "    support_vector_indices = alpha > 1e-5\n",
    "    support_vectors = X[support_vector_indices]\n",
    "    support_vector_labels = y[support_vector_indices]\n",
    "    alpha = alpha[support_vector_indices]\n",
    "\n",
    "    # Compute bias\n",
    "    b = 0\n",
    "    for i in range(len(alpha)):\n",
    "        b += support_vector_labels[i]\n",
    "        b -= np.sum(alpha * support_vector_labels * K[support_vector_indices][i])\n",
    "    b /= len(alpha)\n",
    "    return alpha, support_vectors, support_vector_labels, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_svm_predict(X, alpha, support_vectors, support_vector_labels, b,\n",
    "                      kernel='linear', gamma='scale', degree=3, coef0=0.0):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Features to predict\n",
    "        alpha (numpy.ndarray): Lagrange multipliers\n",
    "        support_vectors (numpy.ndarray): Support vectors\n",
    "        support_vector_labels (numpy.ndarray): Labels of support vectors\n",
    "        b (float): Bias term\n",
    "        kernel (str): Type of kernel\n",
    "        gamma (float): Kernel coefficient\n",
    "        degree (int): Degree of polynomial kernel\n",
    "        coef0 (float): Independent term in kernel function\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted classes (-1 or 1)\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        s = 0\n",
    "        for a, sv_y, sv in zip(alpha, support_vector_labels, support_vectors):\n",
    "            s += a * sv_y * kernel_function(X[i], sv, kernel, gamma, degree, coef0)\n",
    "        y_pred[i] = s + b\n",
    "    return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. Model Training and Evaluation<br>\n",
    "Train both custom and scikit-learn implementations<br>\n",
    "Compare their performance on the test set<br>\n",
    "Generate and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=2220\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m alpha, support_vectors, support_vector_labels, b = \u001b[43mcustom_svm_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrbf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Use RBF kernel for non-linear decision boundary\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Regularization parameter\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscale\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Kernel coefficient\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m custom_predictions = custom_svm_predict(\n\u001b[32m      8\u001b[39m     X_test_scaled, alpha, support_vectors, support_vector_labels, b,\n\u001b[32m      9\u001b[39m     kernel=\u001b[33m'\u001b[39m\u001b[33mrbf\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     gamma=\u001b[33m'\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mcustom_svm_fit\u001b[39m\u001b[34m(X, y, kernel, C, gamma, degree, coef0)\u001b[39m\n\u001b[32m     23\u001b[39m n_samples, n_features = X.shape\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Compute kernel matrix\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m K = \u001b[43mcompute_kernel_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Set up quadratic programming problem\u001b[39;00m\n\u001b[32m     29\u001b[39m P = matrix(np.outer(y, y) * K)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mcompute_kernel_matrix\u001b[39m\u001b[34m(X, kernel, gamma, degree, coef0)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         K[i, j] = \u001b[43mkernel_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mkernel_function\u001b[39m\u001b[34m(x1, x2, kernel, gamma, degree, coef0)\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (gamma * np.dot(x1, x2) + coef0) ** degree\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m kernel == \u001b[33m'\u001b[39m\u001b[33mrbf\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.exp(\u001b[43m-\u001b[49m\u001b[43mgamma\u001b[49m * np.sum((x1 - x2) ** \u001b[32m2\u001b[39m))\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m kernel == \u001b[33m'\u001b[39m\u001b[33msigmoid\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.tanh(gamma * np.dot(x1, x2) + coef0)\n",
      "\u001b[31mTypeError\u001b[39m: bad operand type for unary -: 'str'"
     ]
    }
   ],
   "source": [
    "alpha, support_vectors, support_vector_labels, b = custom_svm_fit(\n",
    "    X_train_scaled, y_train,\n",
    "    kernel='rbf',    # Use RBF kernel for non-linear decision boundary\n",
    "    C=1.0,          # Regularization parameter\n",
    "    gamma='scale'   # Kernel coefficient\n",
    ")\n",
    "custom_predictions = custom_svm_predict(\n",
    "    X_test_scaled, alpha, support_vectors, support_vector_labels, b,\n",
    "    kernel='rbf',\n",
    "    gamma='scale'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model = SVC(\n",
    "    kernel='rbf',    # Use RBF kernel\n",
    "    C=1.0,          # Regularization parameter\n",
    "    gamma='scale'   # Kernel coefficient\n",
    ")\n",
    "sklearn_model.fit(X_train_scaled, y_train)\n",
    "sklearn_predictions = sklearn_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5. Model Evaluation<br>\n",
    "Evaluate model performance using various metrics<br>\n",
    "Compare custom and scikit-learn implementations<br>\n",
    "Print evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCustom Implementation Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, custom_predictions):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, custom_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nScikit-learn Implementation Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, sklearn_predictions):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, sklearn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 6. Visualization<br>\n",
    "Create visualizations to understand model behavior<br>\n",
    "Plot decision boundaries and confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model, support_vectors=None, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary and data points using seaborn.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Features\n",
    "        y (numpy.ndarray): Target values\n",
    "        model: Trained model\n",
    "        support_vectors (numpy.ndarray): Support vectors (optional)\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    h = 0.02  # Step size\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict for each point in the mesh\n",
    "    if hasattr(model, 'predict'):\n",
    "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = custom_svm_predict(\n",
    "            np.c_[xx.ravel(), yy.ravel()],\n",
    "            model[0], model[1], model[2], model[3],\n",
    "            kernel='rbf', gamma='scale'\n",
    "        )\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Create DataFrame for seaborn\n",
    "    df = pd.DataFrame({\n",
    "        'Feature 1': X[:, 0],\n",
    "        'Feature 2': X[:, 1],\n",
    "        'Class': y\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df, x='Feature 1', y='Feature 2', hue='Class', alpha=0.8)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    if support_vectors is not None:\n",
    "        plt.scatter(support_vectors[:, 0], support_vectors[:, 1],\n",
    "                   s=100, linewidth=1, facecolors='none', edgecolors='k',\n",
    "                   label='Support Vectors')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix using seaborn.\n",
    "    \n",
    "    Args:\n",
    "        y_true (numpy.ndarray): True labels\n",
    "        y_pred (numpy.ndarray): Predicted labels\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Class -1', 'Class 1'],\n",
    "                yticklabels=['Class -1', 'Class 1'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X_test, y_test, \n",
    "                      (alpha, support_vectors, support_vector_labels, b),\n",
    "                      support_vectors=support_vectors,\n",
    "                      title=\"Custom Implementation Decision Boundary\")\n",
    "plot_confusion_matrix(y_test, custom_predictions,\n",
    "                     \"Custom Implementation Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X_test, y_test, sklearn_model,\n",
    "                      title=\"Scikit-learn Implementation Decision Boundary\")\n",
    "plot_confusion_matrix(y_test, sklearn_predictions,\n",
    "                     \"Scikit-learn Implementation Confusion Matrix\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
