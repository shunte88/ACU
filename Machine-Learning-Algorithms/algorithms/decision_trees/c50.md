# C5.0

## Overview
C5.0 is an improved version of the C4.5 algorithm that offers better memory efficiency, faster training times, and additional features like boosting. It maintains the core functionality of C4.5 while introducing several enhancements that make it more practical for real-world applications.

## Algorithm Details

### Time Complexity
- Training: O(n * p * log(n)), where n is the number of samples and p is the number of features
- Prediction: O(log(n))
- With boosting: O(n_estimators * n * p * log(n))

### Space Complexity
- O(n * p) for storing the tree structure
- With boosting: O(n_estimators * n * p)

### Key Components

1. **Node Structure**
   - Feature name
   - Threshold value (for continuous features)
   - Dictionary of child nodes
   - Value (for leaf nodes)
   - Feature type indicator
   - Confidence score
   - Number of training cases

2. **Splitting Criterion**
   - Gain Ratio (Information Gain / Split Information)
   - Handles both continuous and categorical features
   - Supports missing values
   - Confidence-based pruning

3. **Stopping Criteria**
   - Maximum tree depth
   - Minimum samples per split
   - Pure node (all samples belong to same class)
   - No more features to split on
   - Minimum confidence threshold

### Algorithm Steps

1. **Initialization**
   - Start with the root node containing all training data
   - Set maximum depth and minimum samples per split
   - Specify feature types (continuous/categorical)
   - Set minimum confidence for pruning

2. **Recursive Splitting**
   - For each node:
     - Calculate gain ratio for each feature
     - For continuous features, find optimal split point
     - Select feature with highest gain ratio
     - Create child nodes based on split
     - Recursively apply to child nodes
     - Stop when stopping criteria met

3. **Leaf Node Assignment**
   - Assign majority class to leaf nodes
   - Calculate confidence score
   - Store number of training cases

4. **Boosting (Optional)**
   - Initialize sample weights
   - For each iteration:
     - Train tree with current weights
     - Calculate tree weight
     - Update sample weights
     - Store tree and weight

## Implementation Details

### Key Methods

1. **`_calculate_entropy(y)`**
   - Calculates entropy of target variable
   - Used for measuring information gain

2. **`_calculate_split_info(X, feature)`**
   - Computes split information for a feature
   - Used in gain ratio calculation

3. **`_calculate_gain_ratio(X, y, feature)`**
   - Computes gain ratio for a feature
   - Combines information gain and split information

4. **`_calculate_confidence(y)`**
   - Calculates confidence score for a node
   - Used for pruning decisions

5. **`_build_tree(X, y, features, depth)`**
   - Recursively constructs decision tree
   - Handles both continuous and categorical features
   - Implements stopping criteria
   - Stores confidence scores

### Usage Example

```python
from c50 import C50

# Initialize model
c50 = C50(
    max_depth=5,
    min_samples_split=2,
    min_confidence=0.25,
    use_boosting=True,
    n_estimators=10
)

# Define feature names and types
feature_names = ['age', 'income', 'education', 'marital_status']
feature_types = ['continuous', 'continuous', 'categorical', 'categorical']

# Train model
c50.fit(X_train, y_train, feature_names, feature_types)

# Make predictions
predictions = c50.predict(X_test)

# Calculate accuracy
accuracy = c50.score(X_test, y_test)
```

## Advantages

1. **Memory Efficiency**
   - More compact tree structure
   - Better memory management
   - Reduced storage requirements

2. **Performance**
   - Faster training times
   - Improved prediction speed
   - Better handling of large datasets

3. **Additional Features**
   - Built-in boosting support
   - Confidence-based pruning
   - Case counting
   - Better handling of missing values

## Limitations

1. **Complexity**
   - More parameters to tune
   - Requires feature type specification
   - More complex implementation

2. **Memory Usage with Boosting**
   - Stores multiple trees
   - Higher memory requirements
   - Longer training time

3. **Parameter Sensitivity**
   - Sensitive to confidence threshold
   - Requires careful tuning
   - May need cross-validation

## Best Practices

1. **Data Preparation**
   - Handle missing values
   - Scale continuous features
   - Encode categorical features
   - Specify feature types correctly

2. **Hyperparameter Tuning**
   - Adjust max_depth
   - Set appropriate min_samples_split
   - Tune min_confidence
   - Choose number of boosting iterations

3. **Validation**
   - Use cross-validation
   - Check for overfitting
   - Monitor confidence scores
   - Evaluate feature importance

## Applications

1. **Classification**
   - Medical diagnosis
   - Credit scoring
   - Customer segmentation
   - Fraud detection

2. **Rule Generation**
   - Business rules
   - Expert systems
   - Decision support
   - Policy making

3. **Feature Selection**
   - Identify important features
   - Understand relationships
   - Reduce dimensionality
   - Feature engineering

## Comparison with Other Algorithms

1. **vs C4.5**
   - More memory efficient
   - Faster training and prediction
   - Built-in boosting support
   - Confidence-based pruning

2. **vs CART**
   - Uses gain ratio instead of Gini impurity
   - Can create multi-way splits
   - More suitable for categorical features
   - Different pruning approach

3. **vs Random Forest**
   - Single tree vs ensemble
   - More interpretable
   - Less robust to noise
   - Faster training and prediction 