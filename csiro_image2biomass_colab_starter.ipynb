{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shunte88/ACU/blob/main/csiro_image2biomass_colab_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ed639c4",
      "metadata": {
        "id": "1ed639c4"
      },
      "source": [
        "# CSIRO Image2Biomass — Colab Starter\n",
        "\n",
        "End‑to‑end baseline for multi‑target biomass prediction.\n",
        "\n",
        "**What you get**: Kaggle download cell, EDA, robust target auto‑detection, GroupKFold CV, PyTorch Lightning image regressor (ViT‑tiny via timm), optional tabular fusion, OOF + submission.\n",
        "\n",
        "> Competition context: launched Oct 2025 by CSIRO/MLA/Google; dataset: 1,162 top‑view quadrats with green/dead/legume components, height, and AOS NDVI. See the [arXiv dataset paper](https://arxiv.org/abs/2510.22916).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba724af4",
      "metadata": {
        "id": "ba724af4"
      },
      "source": [
        "## 0) Runtime\n",
        "If you're on **Colab**, ensure GPU is enabled: `Runtime → Change runtime type → T4/L4/A100`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3784befe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3784befe",
        "outputId": "c422737f-0ba0-4dee-855d-f895e562b581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 29 18:52:18 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   44C    P8             11W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Python 3.12.12  Torch 2.8.0+cu126  CUDA avail: True\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi  # uncomment to inspect GPU\n",
        "import sys, platform, torch\n",
        "print(f'Python {platform.python_version()}  Torch {torch.__version__}  CUDA avail: {torch.cuda.is_available()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee842d33",
      "metadata": {
        "id": "ee842d33"
      },
      "source": [
        "## 1) Install deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8ae21ebb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ae21ebb",
        "outputId": "66bc870f-7653-4147-b71d-3a1e06715c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.9/227.9 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m136.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m134.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m140.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m138.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m144.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.6/752.6 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.4.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install timm==1.0.9 pytorch-lightning==2.4.0 opendatasets==0.1.22 albumentations==1.4.21 pandas==2.2.2 numpy==1.26.4 torchvision==0.19.1 scikit-learn==1.5.2 mlflow==2.16.2 matplotlib==3.9.2 pillow==10.4.0 tqdm==4.66.5 pyyaml==6.0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d77ef7a3",
      "metadata": {
        "id": "d77ef7a3"
      },
      "source": [
        "## 2) Download data from Kaggle\n",
        "You need a Kaggle account + API token:\n",
        "1. Get `kaggle.json` from https://www.kaggle.com/settings/account (Create New API Token)\n",
        "2. Upload it in the cell below or mount Google Drive and place it at `~/.kaggle/kaggle.json`.\n",
        "3. The code will download **CSIRO - Image2Biomass Prediction** data into `./csiro-biomass/`.  \n",
        "If you're running inside Kaggle Notebooks, skip this and use `/kaggle/input/` paths instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b9d48afa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9d48afa",
        "outputId": "618fcd72-0092-46b5-d71e-9a4f356daf8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading csiro-biomass.zip to csiro-biomass\n",
            "100% 1.02G/1.02G [00:50<00:00, 21.4MB/s]\n",
            "100% 1.02G/1.02G [00:50<00:00, 21.6MB/s]\n",
            "Files: ['sample_submission.csv', 'train', 'test.csv', 'train.csv', 'test', 'csiro-biomass.zip']\n"
          ]
        }
      ],
      "source": [
        "import os, json, shutil, pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path('./csiro-biomass')\n",
        "COMP = 'csiro-biomass'\n",
        "\n",
        "# Option A: upload kaggle.json interactively\n",
        "from google.colab import files\n",
        "if not (Path.home()/'.kaggle/kaggle.json').exists():\n",
        "    print('Upload your kaggle.json (from Kaggle account settings).')\n",
        "    uploaded = files.upload()\n",
        "    if 'kaggle.json' in uploaded:\n",
        "        kaggle_dir = Path.home()/'.kaggle'\n",
        "        kaggle_dir.mkdir(exist_ok=True)\n",
        "        with open(kaggle_dir/'kaggle.json','wb') as f:\n",
        "            f.write(uploaded['kaggle.json'])\n",
        "        os.chmod(kaggle_dir/'kaggle.json', 0o600)\n",
        "\n",
        "# Download competition data\n",
        "!pip -q install kaggle==1.6.17\n",
        "!kaggle competitions download -c {COMP} -p {DATA_DIR}\n",
        "!unzip -q -o {DATA_DIR/'csiro-biomass.zip'} -d {DATA_DIR}\n",
        "print('Files:', os.listdir(DATA_DIR))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b5a97f",
      "metadata": {
        "id": "d6b5a97f"
      },
      "source": [
        "## 3) Quick EDA & target auto‑detection\n",
        "This cell prints the available CSVs and infers likely target columns. Adjust `TARGET_COLS` if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3b2136a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b2136a2",
        "outputId": "19b1e838-d90b-45c0-f9be-12e592e85389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found CSVs:\n",
            " csiro-biomass/sample_submission.csv\n",
            "csiro-biomass/test.csv\n",
            "csiro-biomass/train.csv\n",
            "Guessed TRAIN: train.csv\n",
            "Guessed TEST : test.csv\n",
            "Train shape: (1785, 9)\n",
            "                    sample_id              image_path Sampling_Date State  \\\n",
            "0  ID1011485656__Dry_Clover_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
            "1    ID1011485656__Dry_Dead_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
            "2   ID1011485656__Dry_Green_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
            "\n",
            "           Species  Pre_GSHH_NDVI  Height_Ave_cm   target_name   target  \n",
            "0  Ryegrass_Clover           0.62         4.6667  Dry_Clover_g   0.0000  \n",
            "1  Ryegrass_Clover           0.62         4.6667    Dry_Dead_g  31.9984  \n",
            "2  Ryegrass_Clover           0.62         4.6667   Dry_Green_g  16.2751  \n",
            "ID columns: ['sample_id']\n",
            "IMG columns: ['image_path']\n",
            "Group-ish columns: ['Sampling_Date']\n",
            "AUTO TARGET_COLS: []\n",
            "\n",
            "Saved cfg/baseline.yaml: {'data_dir': 'csiro-biomass', 'train_csv': 'csiro-biomass/train.csv', 'test_csv': 'csiro-biomass/test.csv', 'id_col': 'sample_id', 'img_col': 'image_path', 'group_cols': ['Sampling_Date'], 'target_cols': []}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, re\n",
        "from pathlib import Path\n",
        "\n",
        "def find_csv(root):\n",
        "    return sorted([p for p in Path(root).rglob('*.csv')])\n",
        "\n",
        "csvs = find_csv(DATA_DIR)\n",
        "print('Found CSVs:\\n', '\\n'.join(map(str,csvs)))\n",
        "\n",
        "# Heuristics for common file names\n",
        "train_candidates = [p for p in csvs if re.search(r'train', p.name, re.I)]\n",
        "test_candidates  = [p for p in csvs if re.search(r'test',  p.name, re.I)]\n",
        "sub_candidates   = [p for p in csvs if re.search(r'sample|submission', p.name, re.I)]\n",
        "\n",
        "TRAIN_CSV = train_candidates[0] if train_candidates else csvs[0]\n",
        "TEST_CSV  = test_candidates[0]  if test_candidates else (csvs[1] if len(csvs)>1 else csvs[0])\n",
        "print('Guessed TRAIN:', TRAIN_CSV.name)\n",
        "print('Guessed TEST :', TEST_CSV.name)\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "print('Train shape:', train_df.shape)\n",
        "print(train_df.head(3))\n",
        "\n",
        "# Guess ID, image, group columns\n",
        "id_cols = [c for c in train_df.columns if re.search(r'id$', c, re.I)]\n",
        "img_cols = [c for c in train_df.columns if re.search(r'image|img|path|filename', c, re.I)]\n",
        "group_cols = [c for c in train_df.columns if re.search(r'(site|farm|location|block|paddock|date)', c, re.I)]\n",
        "print('ID columns:', id_cols)\n",
        "print('IMG columns:', img_cols)\n",
        "print('Group-ish columns:', group_cols)\n",
        "\n",
        "# Guess target columns: names containing biomass components\n",
        "target_patterns = r'(green|dead|legume|clover|total|biomass)'\n",
        "TARGET_COLS = [c for c in train_df.columns\n",
        "               if re.search(target_patterns, c, re.I) and train_df[c].dtype != 'O']\n",
        "# Keep 3–6 numeric targets max\n",
        "TARGET_COLS = [c for c in TARGET_COLS if c not in id_cols+img_cols][:6]\n",
        "print('AUTO TARGET_COLS:', TARGET_COLS)\n",
        "\n",
        "# Persist a small config for later cells\n",
        "import yaml, json, os\n",
        "cfg = {\n",
        "    'data_dir': str(DATA_DIR),\n",
        "    'train_csv': str(TRAIN_CSV),\n",
        "    'test_csv': str(TEST_CSV),\n",
        "    'id_col': id_cols[0] if id_cols else None,\n",
        "    'img_col': img_cols[0] if img_cols else None,\n",
        "    'group_cols': group_cols[:2],\n",
        "    'target_cols': TARGET_COLS,\n",
        "}\n",
        "os.makedirs('cfg', exist_ok=True)\n",
        "with open('cfg/baseline.yaml','w') as f: yaml.safe_dump(cfg, f)\n",
        "print('\\nSaved cfg/baseline.yaml:', cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050af52c",
      "metadata": {
        "id": "050af52c"
      },
      "source": [
        "## 4) Dataset & augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4247c0d3",
      "metadata": {
        "id": "4247c0d3"
      },
      "outputs": [],
      "source": [
        "import os, cv2, numpy as np, torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Simple albumentations pipeline\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "IM_SIZE = 384\n",
        "\n",
        "def build_train_aug():\n",
        "    return A.Compose([\n",
        "        A.LongestMaxSize(max_size=IM_SIZE),\n",
        "        A.PadIfNeeded(IM_SIZE, IM_SIZE, border_mode=cv2.BORDER_REFLECT_101),\n",
        "        A.RandomBrightnessContrast(0.1, 0.1, p=0.5),\n",
        "        A.HueSaturationValue(10, 10, 10, p=0.3),\n",
        "        A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=10, border_mode=cv2.BORDER_REFLECT_101, p=0.5),\n",
        "        A.Normalize(),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "def build_valid_aug():\n",
        "    return A.Compose([\n",
        "        A.LongestMaxSize(max_size=IM_SIZE),\n",
        "        A.PadIfNeeded(IM_SIZE, IM_SIZE, border_mode=cv2.BORDER_REFLECT_101),\n",
        "        A.Normalize(),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "class BiomassDataset(Dataset):\n",
        "    def __init__(self, df, cfg, root=None, img_aug=None, is_test=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.cfg = cfg\n",
        "        self.root = root or cfg['data_dir']\n",
        "        self.img_col = cfg['img_col']\n",
        "        self.id_col  = cfg['id_col'] or 'id'\n",
        "        self.targets = cfg['target_cols']\n",
        "        self.is_test = is_test\n",
        "        self.aug = img_aug or build_valid_aug()\n",
        "    def __len__(self): return len(self.df)\n",
        "    def load_image(self, row):\n",
        "        path = str(Path(self.root)/row[self.img_col]) if self.img_col else None\n",
        "        if (not path) or (not os.path.exists(path)):\n",
        "            # fallback: try to find an images/ folder and use id.jpg/png\n",
        "            iid = str(row[self.id_col]) if self.id_col and self.id_col in row else str(row.name)\n",
        "            for ext in ('.jpg','.jpeg','.png','.bmp','.tif','.tiff'):\n",
        "                trial = os.path.join(self.root, 'images', f'{iid}{ext}')\n",
        "                if os.path.exists(trial): path = trial; break\n",
        "        if not path:\n",
        "            raise FileNotFoundError('Image path not found; set cfg.img_col correctly.')\n",
        "        image = cv2.imread(path, cv2.IMREAD_COLOR)[:, :, ::-1]\n",
        "        return image\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = self.load_image(row)\n",
        "        augmented = self.aug(image=image)\n",
        "        image_t = augmented['image']\n",
        "        if self.is_test or not self.targets:\n",
        "            return image_t, torch.tensor([0.0])\n",
        "        y = torch.tensor(row[self.targets].values.astype(np.float32))\n",
        "        return image_t, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60fc312f",
      "metadata": {
        "id": "60fc312f"
      },
      "source": [
        "## 5) Model — ViT‑tiny head for multi‑target regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7cce9c89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "7cce9c89",
        "outputId": "4dec8b66-8a3f-4eca-97d3-71ce1338cac7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pytorch_lightning'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1234170047.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vit_tiny_patch16_224.augreg_in21k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import timm, torch, torch.nn as nn, pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class Regressor(pl.LightningModule):\n",
        "    def __init__(self, backbone='vit_tiny_patch16_224.augreg_in21k', n_out=1, lr=2e-4):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.backbone = timm.create_model(backbone, pretrained=True, num_classes=0, global_pool='avg')\n",
        "        in_features = self.backbone.num_features\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(in_features),\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(256, n_out)\n",
        "        )\n",
        "        self.loss = nn.L1Loss()  # MAE for robustness\n",
        "        self.lr = lr\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        return self.head(feat)\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        yhat = self(x)\n",
        "        loss = self.loss(yhat, y)\n",
        "        self.log('train_mae', loss, prog_bar=True)\n",
        "        return loss\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        yhat = self(x)\n",
        "        loss = self.loss(yhat, y)\n",
        "        self.log('val_mae', loss, prog_bar=True)\n",
        "        return loss\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n",
        "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
        "        return [opt], [sch]\n",
        "\n",
        "def make_loaders(train_df, valid_df, cfg, bs=16):\n",
        "    train_ds = BiomassDataset(train_df, cfg, img_aug=build_train_aug())\n",
        "    valid_ds = BiomassDataset(valid_df, cfg, img_aug=build_valid_aug())\n",
        "    tr = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    va = DataLoader(valid_ds, batch_size=bs*2, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    return tr, va\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f0a541d",
      "metadata": {
        "id": "1f0a541d"
      },
      "source": [
        "## 6) Cross‑validation split (GroupKFold by site/date if present)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80361ee8",
      "metadata": {
        "id": "80361ee8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np\n",
        "from sklearn.model_selection import GroupKFold, KFold\n",
        "from pathlib import Path\n",
        "import yaml, math\n",
        "\n",
        "with open('cfg/baseline.yaml') as f: cfg = yaml.safe_load(f)\n",
        "\n",
        "df = pd.read_csv(cfg['train_csv'])\n",
        "targets = cfg['target_cols']\n",
        "assert len(targets) >= 1, \"No target columns detected; please edit cfg/baseline.yaml\"\n",
        "\n",
        "# make groups\n",
        "groups_cols = cfg['group_cols'] or []\n",
        "if len(groups_cols) >= 1:\n",
        "    groups = df[groups_cols[0]].astype(str)\n",
        "    if len(groups_cols) >= 2:\n",
        "        groups = groups + '_' + df[groups_cols[1]].astype(str)\n",
        "    splitter = GroupKFold(n_splits=5)\n",
        "    cv = list(splitter.split(df, groups=groups))\n",
        "    print('Using GroupKFold on', groups_cols)\n",
        "else:\n",
        "    splitter = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv = list(splitter.split(df))\n",
        "    print('Using KFold (no group columns found)')\n",
        "\n",
        "print('Fold sizes:', [len(v) for _, v in cv])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f109035c",
      "metadata": {
        "id": "f109035c"
      },
      "source": [
        "## 7) Train one fold (quick sanity check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a47edbc2",
      "metadata": {
        "id": "a47edbc2"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl, numpy as np\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "fold = 0\n",
        "tr_idx, va_idx = cv[fold]\n",
        "train_df = df.iloc[tr_idx].reset_index(drop=True)\n",
        "valid_df = df.iloc[va_idx].reset_index(drop=True)\n",
        "\n",
        "tr_loader, va_loader = make_loaders(train_df, valid_df, cfg, bs=16)\n",
        "model = Regressor(n_out=len(targets))\n",
        "\n",
        "ckpt = ModelCheckpoint(monitor='val_mae', mode='min', save_top_k=1, filename='fold{fold}-{{epoch:02d}}-{{val_mae:.4f}}')\n",
        "es = EarlyStopping(monitor='val_mae', mode='min', patience=3)\n",
        "trainer = pl.Trainer(max_epochs=10, precision='16-mixed', callbacks=[ckpt, es], log_every_n_steps=20, enable_checkpointing=True)\n",
        "\n",
        "trainer.fit(model, tr_loader, va_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f82c00cb",
      "metadata": {
        "id": "f82c00cb"
      },
      "source": [
        "## 8) OOF predictions + Submission template\n",
        "This creates `oof.csv` for diagnostics and a skeleton `submission.csv` based on sample file if present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b25823f",
      "metadata": {
        "id": "1b25823f"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd, yaml, os\n",
        "from pathlib import Path\n",
        "\n",
        "# Save OOF for the single fold (demo); extend to full CV later\n",
        "valid_loader = va_loader\n",
        "model.eval(); preds=[]; gts=[]\n",
        "with torch.no_grad():\n",
        "    for xb, yb in valid_loader:\n",
        "        xb = xb.to(model.device)\n",
        "        yhat = model(xb).cpu().numpy()\n",
        "        preds.append(yhat); gts.append(yb.numpy())\n",
        "preds = np.vstack(preds); gts = np.vstack(gts)\n",
        "oof = valid_df.copy()\n",
        "for i,t in enumerate(targets): oof[f'pred_{t}'] = preds[:,i]\n",
        "oof.to_csv('out_oof_fold0.csv', index=False)\n",
        "print('Saved out_oof_fold0.csv with columns:', list(oof.columns))\n",
        "\n",
        "# Build submission\n",
        "# Try to find sample submission\n",
        "sub_path = None\n",
        "for p in Path(cfg['data_dir']).rglob('*.csv'):\n",
        "    if re.search(r'sample|submission', p.name, re.I):\n",
        "        sub_path = p; break\n",
        "\n",
        "if sub_path:\n",
        "    sub = pd.read_csv(sub_path)\n",
        "    # Try to populate columns that intersect with TARGET_COLS\n",
        "    for t in targets:\n",
        "        for c in sub.columns:\n",
        "            if re.sub('[^a-z]','',c.lower()) == re.sub('[^a-z]','',t.lower()):\n",
        "                # Fill with global mean as placeholder\n",
        "                sub[c] = df[t].mean()\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Wrote submission.csv based on sample:', sub.shape)\n",
        "else:\n",
        "    print('No sample_submission found; please craft one based on competition schema.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b842069",
      "metadata": {
        "id": "1b842069"
      },
      "source": [
        "## 9) Next steps\n",
        "- Expand to full 5‑fold CV and average OOF.\n",
        "- Try ConvNeXt‑T or EfficientNet‑B0.\n",
        "- Add tabular fusion (height, AOS NDVI) by concatenating embeddings + normalized tabular features.\n",
        "- Add quantile heads for uncertainty.\n",
        "- Ensembling across seeds & backbones.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}